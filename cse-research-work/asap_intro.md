# ASAp Algorithm: Adaptive Sampling with Approximate Expected Futures

## Overview

The _ASAp (Adaptive Sampling with Approximate Expected Futures)_ algorithm is a decoding method designed for Grammar-Aligned Decoding (GAD). It ensures that outputs generated by a language model (LM) adhere to a given grammar while preserving the model's probability distribution.

_Why do we need this?_

Traditional Grammar-Constrained Decoding (GCD) methods enforce grammatical correctness but introduce bias, leading to outputs that deviate from the model's true distribution.

The limitations can be summarized as,

- GCD masks tokens that lead to ungrammatical outputs.
- Does not consider the probability of future grammaticality.
- Biases the output, sometimes generating unlikely but grammatically correct sequences.

ASAp iteratively refines its approximation of Expected Future Grammaticality (EFG) to produce more accurate and unbiased outputs.

## Problem Statement

Given a language model (LM) with a probability distribution $P$ and a context-free grammar (CFG) $G$, we want to sample sequences that,

1. Adhere to the grammar - Ensure that all generated outputs belong to the grammar’s language $L(G)$.
2. Preserve the LM’s learned probabilities - Maintain a probability distribution proportional to the LM’s original distribution.

This corresponds to sampling from,

$$ Q*{P,G}(w) = \frac{1[w \in L(G)] \cdot P(w)}{\sum*{w'} 1[w' \in L(G)] \cdot P(w')} $$

where $1[w \in L(G)]$ ensures grammatical correctness.

## ASAp Algorithm

The ASAp algorithm iteratively improves the estimation of EFG to reduce GCD’s bias. It starts with a rough approximation and refines it as more samples are generated.

### Key Steps

1. Initialization:

   - Start with an empty set of samples $S$.
   - Initialize an over-approximation of expected future grammaticality, $\tilde{c}_S(w_{1:i})$.

2. Iterative Refinement:

   - Sample a sequence $w$ from the constrained distribution $Q$.
   - Update $\tilde{c}_S(w_{1:i})$ for each prefix using past observations:

     $$ \tilde{c}_S(w_{1:i}) = \sum*{w*{i+1}} P(w*{i+1} | w*{1:i}) \cdot \tilde{c}_S(w_{1:i+1}) $$

   - Propagate these updates backward through the sequence.

3. Convergence:
   - Over multiple iterations, $\tilde{c}_S(w_{1:i})$ converges to the true EFG.
   - Sampling becomes more accurate and aligns with the LM’s probability distribution.

## ASAp vs. GCD

| Feature                    | GCD                  | ASAp                     |
| -------------------------- | -------------------- | ------------------------ |
| Ensures grammaticality     | Yes                  | Yes                      |
| Preserves LM probabilities | No (bias introduced) | Yes (improves over time) |
| Computational efficiency   | High                 | Moderate                 |
| Iterative refinement       | No                   | Yes                      |
| Guaranteed convergence     | No                   | Yes                      |

## Insights

- reduces KL divergence between its output distribution and the LM’s true distribution faster than GCD.
- produces more likely outputs while maintaining grammatical correctness.
- convergence is slower compared to GCD but results in higher accuracy over multiple iterations.

## Summary

ASAp provides a principled approach to grammar-aligned decoding, correcting the bias in GCD by iteratively refining expected future grammaticality estimates. This ensures that the output remains faithful to both the grammar and the language model’s learned probability distribution.
